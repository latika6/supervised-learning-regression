{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week 2\n",
    "# machine learning life cycles\n",
    "# feature interaction\n",
    "# gradient descent\n",
    "# feature selection\n",
    "# hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning life cycles\n",
    "# 1. data processing - problem statement, dataset, eda, collection, formatting\n",
    "# 2. feature selection - independent, target variables, feature transformation, engineering, selection\n",
    "# 3. modeling - model builiding, selection of algo\n",
    "# 4. optimization - performance of the models, fine tuning, validation\n",
    "# 5. deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent is to help to reduce cost function \n",
    "# Batch Gradient Descent - \n",
    "# Cost Function = loss function = error function = sum of squares errors\n",
    "# E1 = actual - predicted\n",
    "# E2\n",
    "# if E2 < E1 then E2 is the best fit line for the model\n",
    "# learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of gradient descent\n",
    "# learning rate (alpha) - minimise the probability of algorithm iteration\n",
    "# it is always a positive \n",
    "# near to zero\n",
    "# types of Gradient Desecent\n",
    "# batch gradient descent - we don't use because of the local minimum (faster)\n",
    "# stochastic - every record (it is very slow), we do for large datasets\n",
    "# mini-batch - we can stuck between local minimum (faster) (mid of full batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection Theory\n",
    "# also known for variable selection \n",
    "# scatter plot\n",
    "# forward selection method\n",
    "# backward elimination method\n",
    "# stepwise method\n",
    "\n",
    "# Forward \n",
    "# null value\n",
    "# with highest correlation there we build the model\n",
    "# build another models\n",
    "# check p value like null or alternative hypothesis\n",
    "# p value should not be change, but actually change while doing\n",
    "\n",
    "# Backward\n",
    "# build single model\n",
    "# remove with least significant correlation\n",
    "# try with other variables\n",
    "\n",
    "# stepwise\n",
    "# start with null\n",
    "# remove varibale based on p value\n",
    "# stop when no varibale can be added or removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "# train full model\n",
    "# create subsets\n",
    "# compute ranking criteria\n",
    "# remove least ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Transformation and Types\n",
    "# tranform weak feature into good feature for the model\n",
    "# to reduce the error terms\n",
    "# make data into linear line\n",
    "# assumptions\n",
    "# make normal distribution\n",
    "# there are many methods\n",
    "# Logarithmic tranformation - natural log, numerical, above 0\n",
    "# Square Root Transformation - with some certain range\n",
    "# Reciprocal transformation - 1/ value\n",
    "# to reduce the right skewed data we use square root transformation\n",
    "# Exponential Transformation - we applu exponential function on natural log\n",
    "# Box-cox transformation - statistical - positive variables only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fearure Scaling\n",
    "# Normalization (0 - 1) - we use if the data is not normally distributed\n",
    "# Standardization - normally distributed\n",
    "# Normalization and standardization are two common techniques used to transform the features of a dataset. \n",
    "# Normalization scales the features to a range of [0, 1], which can be useful when the range of the features varies widely. \n",
    "# Standardization, on the other hand, scales the features to have mean 0 and variance 1, which can be useful when the features have different units or scales. \n",
    "# Therefore, the correct answer is A, as normalization scales the features to a range of [0, 1] while standardization scales the features to have mean 0 and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Optimization\n",
    "# bias error - in control - underfit\n",
    "# variance error - in control - overfit\n",
    "# random error - not under control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Validation Techniques\n",
    "# 70:30\n",
    "# cross validation \n",
    "# k = 2 that means data split into 2\n",
    "# train and test\n",
    "# k - fold cross validation\n",
    "# LOOCV (data split into 100 pieces)\n",
    "# Grid Search\n",
    "# eta and learning rate\n",
    "# K-fold cross-validation is a technique used to assess the performance of a machine learning model. \n",
    "# It involves dividing the data into K subsets, or folds, and using K-1 of them to train the model while the remaining one is used to test it. \n",
    "# This process is repeated K times, with each fold being used as the test set once. \n",
    "# The performance of the model is then averaged across all K iterations to provide a more accurate estimate of its overall performance. \n",
    "# K-fold cross-validation helps to prevent overfitting, which is when a model performs well on the training data but poorly on new, unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
